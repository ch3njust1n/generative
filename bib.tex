%%% Metaprogramming

% 2012
@book{vstuikys2012meta,
  title={Meta-programming and model-driven meta-program development: principles, processes and techniques},
  author={{\v{S}}tuikys, Vytautas and Dama{\v{s}}evi{\v{c}}ius, Robertas},
  volume={5},
  year={2012},
  publisher={Springer Science \& Business Media}
}

% 2022
@book{aloorravi2022metaprogramming,
  title={Metaprogramming with Python},
  author={AloorRavi, Sulekha},
  isbn={9781838554651},
  year={2022},
  publisher={Packt Publishing}
}


%%% Code Repair

% 2017
@inproceedings{tan2017codeflaws,
  title={Codeflaws: a programming competition benchmark for evaluating automated program repair tools},
  author={Tan, Shin Hwei and Yi, Jooyong and Mechtaev, Sergey and Roychoudhury, Abhik and others},
  booktitle={2017 IEEE/ACM 39th International Conference on Software Engineering Companion (ICSE-C)},
  pages={180--182},
  year={2017},
  organization={IEEE}
}


% 2020
@article{bieber2020learning,
  title={Learning to execute programs with instruction pointer attention graph neural networks},
  author={Bieber, David and Sutton, Charles and Larochelle, Hugo and Tarlow, Daniel},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8626--8637},
  year={2020}
}

@article{karampatsis2020scelmo,
  title={Scelmo: Source code embeddings from language models},
  author={Karampatsis, Rafael-Michael and Sutton, Charles},
  journal={arXiv preprint arXiv:2004.13214},
  year={2020}
}

@article{mousavi2020obstacles,
  title={Obstacles in Fully Automatic Program Repair: A survey},
  author={Mousavi, S Amirhossein and Babani, Donya Azizi and Flammini, Francesco},
  journal={arXiv preprint arXiv:2011.02714},
  year={2020}
}


% 2021
@article{wang2021codet5,
  title={Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation},
  author={Wang, Yue and Wang, Weishi and Joty, Shafiq and Hoi, Steven CH},
  journal={arXiv preprint arXiv:2109.00859},
  year={2021}
}

@misc{austin2021program,
      title={Program Synthesis with Large Language Models},
      author={Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and Charles Sutton},
      year={2021},
      eprint={2108.07732},
      archivePrefix={arXiv},
      primaryClass={cs.PL}
}


% 2022
@article{bui2022detect,
  title={Detect-Localize-Repair: A Unified Framework for Learning to Debug with CodeT5},
  author={Bui, Nghi DQ and Wang, Yue and Hoi, Steven},
  journal={arXiv preprint arXiv:2211.14875},
  year={2022}
}

@inproceedings{grishina2022enabling,
  title={Enabling automatic repair of source code vulnerabilities using data-driven methods},
  author={Grishina, Anastasiia},
  booktitle={Proceedings of the ACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings},
  pages={275--277},
  year={2022}
}

@inproceedings{jain2022jigsaw,
author = {Jain, Naman and Vaidyanath, Skanda and Iyer, Arun and Natarajan, Nagarajan and Parthasarathy, Suresh and Rajamani, Sriram and Sharma, Rahul},
title = {Jigsaw: Large Language Models meet Program Synthesis},
booktitle = {International Conference on Software Engineering (ICSE)},
year = {2022},
month = {May},
abstract = {Large pre-trained language models such as GPT-3, Codex, and Google's language model are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, jigsaw has an important role to play in improving the accuracy of the systems.},
url = {https://www.microsoft.com/en-us/research/publication/jigsaw-large-language-models-meet-program-synthesis/},
}

@article{joshi2022repair,
  title={Repair is nearly generation: Multilingual program repair with llms},
  author={Joshi, Harshit and Cambronero, Jos{\'e} and Gulwani, Sumit and Le, Vu and Radicek, Ivan and Verbruggen, Gust},
  journal={arXiv preprint arXiv:2208.11640},
  year={2022}
}

@article{liu2022adaptivepaste,
  title={AdaptivePaste: Code Adaptation through Learning Semantics-aware Variable Usage Representations},
  author={Liu, Xiaoyu and Jang, Jinu and Sundaresan, Neel and Allamanis, Miltiadis and Svyatkovskiy, Alexey},
  journal={arXiv preprint arXiv:2205.11023},
  year={2022}
}

@misc{ni2023lever,
      title={LEVER: Learning to Verify Language-to-Code Generation with Execution},
      author={Ansong Ni and Srini Iyer and Dragomir Radev and Ves Stoyanov and Wen-tau Yih and Sida I. Wang and Xi Victoria Lin},
      year={2023},
      eprint={2302.08468},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{nijkamp2022codegen,
  title={Codegen: An open large language model for code with multi-turn program synthesis},
  author={Nijkamp, Erik and Pang, Bo and Hayashi, Hiroaki and Tu, Lifu and Wang, Huan and Zhou, Yingbo and Savarese, Silvio and Xiong, Caiming},
  journal={arXiv preprint arXiv:2203.13474},
  year={2022}
}

@misc{perez2022red,
      title={Red Teaming Language Models with Language Models},
      author={Ethan Perez and Saffron Huang and Francis Song and Trevor Cai and Roman Ring and John Aslanides and Amelia Glaese and Nat McAleese and Geoffrey Irving},
      year={2022},
      eprint={2202.03286},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{zirak2022improving,
  title={Improving Automated Program Repair with Domain Adaptation},
  author={Zirak, Armin and Hemati, Hadi},
  journal={arXiv preprint arXiv:2212.11414},
  year={2022}
}

@inproceedings{zhong2022neural,
  title={Neural Program Repair: Systems, Challenges and Solutions},
  author={Zhong, Wenkang and Li, Chuanyi and Ge, Jidong and Luo, Bin},
  booktitle={Proceedings of the 13th Asia-Pacific Symposium on Internetware},
  pages={96--106},
  year={2022}
}

% 2023
@article{ahmed2023majority,
  title={Majority Rule: better patching via Self-Consistency},
  author={Ahmed, Toufique and Devanbu, Premkumar},
  journal={arXiv preprint arXiv:2306.00108},
  year={2023}
}


@article{chen2023teaching,
  title={Teaching large language models to self-debug},
  author={Chen, Xinyun and Lin, Maxwell and Sch{\"a}rli, Nathanael and Zhou, Denny},
  journal={arXiv preprint arXiv:2304.05128},
  year={2023}
}

@article{fan2022automated,
  title={Automated Repair of Programs from Large Language Models},
  author={Fan, Zhiyu and Gao, Xiang and Roychoudhury, Abhik and Tan, Shin Hwei},
  journal={arXiv preprint arXiv:2205.10583},
  year={2022}
}

@article{first2023baldur,
  title={Baldur: Whole-Proof Generation and Repair with Large Language Models},
  author={First, Emily and Rabe, Markus N and Ringer, Talia and Brun, Yuriy},
  journal={arXiv preprint arXiv:2303.04910},
  year={2023}
}

@article{khanfir2023ibir,
  title={iBiR: Bug-report-driven fault injection},
  author={Khanfir, Ahmed and Koyuncu, Anil and Papadakis, Mike and Cordy, Maxime and Bissyand{\'e}, Tegawend{\'e} F and Klein, Jacques and Le Traon, Yves},
  journal={ACM Transactions on Software Engineering and Methodology},
  volume={32},
  number={2},
  pages={1--31},
  year={2023},
  publisher={ACM New York, NY}
}

@article{liventsev2023fully,
  title={Fully Autonomous Programming with Large Language Models},
  author={Liventsev, Vadim and Grishina, Anastasiia and H{\"a}rm{\"a}, Aki and Moonen, Leon},
  journal={arXiv preprint arXiv:2304.10423},
  year={2023}
}

@article{tian2023chatgpt,
  title={Is ChatGPT the Ultimate Programming Assistant--How far is it?},
  author={Tian, Haoye and Lu, Weiqi and Li, Tsz On and Tang, Xunzhu and Cheung, Shing-Chi and Klein, Jacques and Bissyand{\'e}, Tegawend{\'e} F},
  journal={arXiv preprint arXiv:2304.11938},
  year={2023}
}

@article{wu2023effective,
  title={How Effective Are Neural Networks for Fixing Security Vulnerabilities},
  author={Wu, Yi and Jiang, Nan and Pham, Hung Viet and Lutellier, Thibaud and Davis, Jordan and Tan, Lin and Babkin, Petr and Shah, Sameena},
  journal={arXiv preprint arXiv:2305.18607},
  year={2023}
}

@inproceedings{xia2023automated,
  title={Automated program repair in the era of large pre-trained language models},
  author={Xia, Chunqiu Steven and Wei, Yuxiang and Zhang, Lingming},
  booktitle={Proceedings of the 45th International Conference on Software Engineering (ICSE 2023). Association for Computing Machinery},
  year={2023}
}



%%%% Large Language Models

% 2021
@misc{brundage2022lessons,
  title={Lessons learned on language model safety and misuse},
  author={Brundage, Miles and Mayer, Katie and Eloundou, Tyna and Agarwal, Sandhini and Adler, Steven and Krueger, Gretchen and Leike, Jan and Mishkin, Pamela},
  year={2022},
  publisher={OpenAI. https://perma. cc/8RKR-QJZY}
}

@article{gao2022pal,
  title={PAL: Program-aided Language Models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  journal={arXiv preprint arXiv:2211.10435},
  year={2022}
}

@article{parisi2022talm,
  title={Talm: Tool augmented language models},
  author={Parisi, Aaron and Zhao, Yao and Fiedel, Noah},
  journal={arXiv preprint arXiv:2205.12255},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

% 2023

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{cai2023large,
  title={Large Language Models as Tool Makers},
  author={Cai, Tianle and Wang, Xuezhi and Ma, Tengyu and Chen, Xinyun and Zhou, Denny},
  journal={arXiv preprint arXiv:2305.17126},
  year={2023}
}

@article{greshake2023more,
  title={More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  journal={arXiv preprint arXiv:2302.12173},
  year={2023}
}

@article{hao2023toolkengpt,
  title={ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings},
  author={Hao, Shibo and Liu, Tianyang and Wang, Zhen and Hu, Zhiting},
  journal={arXiv preprint arXiv:2305.11554},
  year={2023}
}

@misc{huyen2023building,
  author = {Huyen, Chip},
  title = {Building LLM applications for production},
  year = {2023},
  month = {4},
  howpublished = {\url{https://huyenchip.com/2023/04/11/llm-engineering.html}},
  note = {Accessed: 2023-06-02}
}

@article{liang2023taskmatrix,
  title={Taskmatrix. ai: Completing tasks by connecting foundation models with millions of apis},
  author={Liang, Yaobo and Wu, Chenfei and Song, Ting and Wu, Wenshan and Xia, Yan and Liu, Yu and Ou, Yang and Lu, Shuai and Ji, Lei and Mao, Shaoguang and others},
  journal={arXiv preprint arXiv:2303.16434},
  year={2023}
}

@article{kang2023exploiting,
  title={Exploiting programmatic behavior of llms: Dual-use through standard security attacks},
  author={Kang, Daniel and Li, Xuechen and Stoica, Ion and Guestrin, Carlos and Zaharia, Matei and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2302.05733},
  year={2023}
}

@article{kim2023language,
  title={Language models can solve computer tasks},
  author={Kim, Geunwoo and Baldi, Pierre and McAleer, Stephen},
  journal={arXiv preprint arXiv:2303.17491},
  year={2023}
}

@article{lu2023bounding,
  title={Bounding the Capabilities of Large Language Models in Open Text Generation with Prompt Constraints},
  author={Lu, Albert and Zhang, Hongxin and Zhang, Yanzhe and Wang, Xuezhi and Yang, Diyi},
  journal={arXiv preprint arXiv:2302.09185},
  year={2023}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={arXiv preprint arXiv:2303.17651},
  year={2023}
}

@article{paranjape2023art,
  title={ART: Automatic multi-step reasoning and tool-use for large language models},
  author={Paranjape, Bhargavi and Lundberg, Scott and Singh, Sameer and Hajishirzi, Hannaneh and Zettlemoyer, Luke and Ribeiro, Marco Tulio},
  journal={arXiv preprint arXiv:2303.09014},
  year={2023}
}

@article{patil2023gorilla,
  title={Gorilla: Large Language Model Connected with Massive APIs},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={arXiv preprint arXiv:2305.15334},
  year={2023}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={arXiv preprint arXiv:2302.04761},
  year={2023}
}

@article{shinn2023reflexion,
  title={Reflexion: an autonomous agent with dynamic memory and self-reflection},
  author={Shinn, Noah and Labash, Beck and Gopinath, Ashwin},
  journal={arXiv preprint arXiv:2303.11366},
  year={2023}
}

@article{wang2023voyager,
  title={Voyager: An Open-Ended Embodied Agent with Large Language Models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2305.16291},
  year={2023}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@inproceedings{52294,
title	= {Generative Agents: Interactive Simulacra of Human Behavior},
author	= {Joon Sung Park and Joseph C. O'Brien and Carrie Cai and Meredith Ringel Morris and Percy Liang and Michael Bernstein},
year	= {2023},
URL	= {https://arxiv.org/pdf/2304.03442.pdf}
}

@misc{Chase2023,
  author = "hwchase17",
  title = "langchain: Building applications with LLMs through composability",
  year = "2023",
  url = "https://github.com/hwchase17/langchain",
  note = "commit 20e9ce8"
}

@misc{goodside_2023,
  author = {Goodside, Riley},
  title = {I, too, am an AI expert. I make it say “poop.”},
  year = {2023},
  howpublished = {Tweet},
  month = {June 2},
  url = {https://twitter.com/goodside/status/1664750905588056065}
}

@misc{karpathy2023state,
  author = {Karpathy, Andrej},
  title = {State of GPT},
  howpublished = {Talk presented at Microsoft Build},
  year = {2023},
  note = {Available at: \url{https://www.youtube.com/watch?v=bZQun8Y4L2A}}
}

@misc{Nin_kat2023,
    author = "Nin_kat",
    title = "New jailbreak based on virtual functions - smuggle illegal tokens to the backend",
    year = "2023",
    url = "https://www.reddit.com/r/ChatGPT/comments/10urbdj/new_jailbreak_based_on_virtual_functions_smuggle",
    note = "Accessed: Jun 8, 2023",
    howpublished = "\url{https://www.reddit.com/r/ChatGPT/comments/10urbdj/new_jailbreak_based_on_virtual_functions_smuggle}"
}

@misc{openai_cookbook_2023,
  author = {OpenAI},
  title = {OpenAI Cookbook: Techniques to Improve Reliability},
  year = {2023},
  url = {https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md},
  note = {Accessed: June 2, 2023}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report},
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Willison2023,
  author = "Simon Willison",
  title = "The Dual LLM pattern for building AI assistants that can resist prompt injection",
  year = "2023",
  url = "https://simonwillison.net/2023/Apr/25/dual-llm-pattern/"
}


%%% ML OpSec

@misc{Klondike2023,
  author = "Gavin Klondike",
  title = "Threat Modeling LLM Applications",
  year = "2023",
  url = "http://aivillage.org/large%20language%20models/threat-modeling-llm/",
  publisher = "AI Village"
}

@misc{arp2021dos,
      title={Dos and Don'ts of Machine Learning in Computer Security},
      author={Daniel Arp and Erwin Quiring and Feargus Pendlebury and Alexander Warnecke and Fabio Pierazzi and Christian Wressnegger and Lorenzo Cavallaro and Konrad Rieck},
      year={2021},
      eprint={2010.09470},
      archivePrefix={arXiv},
      primaryClass={cs.CR}
}